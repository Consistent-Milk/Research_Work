{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __XOR__ function takes in two binary values as input, and it can summarized as below:\n",
    "\n",
    "$$ f(x_1, x_2)=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0, \\hspace{0.2cm} if \\hspace{0.2cm} x_1 = 0 \\hspace{0.2cm} and \\hspace{0.2cm} x_2 = 0\\\\\n",
    "      1, \\hspace{0.2cm} if \\hspace{0.2cm} x_1 = 1 \\hspace{0.2cm} and \\hspace{0.2cm} x_2 = 0\\\\\n",
    "      1, \\hspace{0.2cm} if \\hspace{0.2cm} x_1 = 0 \\hspace{0.2cm} and \\hspace{0.2cm} x_2 = 1\\\\\n",
    "      0, \\hspace{0.2cm} if \\hspace{0.2cm} x_1 = 1 \\hspace{0.2cm} and \\hspace{0.2cm} x_2 = 1\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that the XOR function provides us the target function $y = f^{*}(x)$ that we want to learn. \n",
    "\n",
    "Let our model be the function defined by $y = f(x; \\theta)$ and our learning algorithm will adapt the parameters $\\theta$ to make $f$ as similar as possible to $f^{*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our model to perform correctly on the four points $X = \\bigl\\{[0,0]^{T}, [0,1]^{T}, [1,0]^{T}, [1,1]^{T}\\bigl\\}$.\n",
    "\n",
    "We can treat this problem as a regression problem and use mean squared error (MSE) as our loss function. Evaluated on the whole dataset the MSE loss function is,\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4} \\sum_{x \\in X} (f^{*}(x) - f(x; \\theta))^{2}\n",
    "$$\n",
    "\n",
    "which we get from the main form the MSE loss,\n",
    "\n",
    "$$\n",
    "MSE_{test} = \\frac{1}{m} \\sum_{i} (\\hat{y}^{(test)} - y^{(test)})^{2}_{i} = \\frac{1}{m} \\sum_{i} \\big\\| (\\hat{y}^{(test)} - y^{(test)}) \\big\\|^{2}_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will choose the form of our model $f(x;\\theta)$. Suppose that we choose a linear model, with $\\theta$ consisting of $w$ and $b$. Our model is then defined to be,\n",
    "$$\n",
    "f(x;w, b) = x^{T}w + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation\n",
    "\n",
    "To minizime the loss function we will use the normal equations. Here we derive the normal equations for the linear model,\n",
    "$$\n",
    "\\hat{y} = w^{T}x\n",
    "$$\n",
    "\n",
    "To minizime $MSE_{train}$ we will solve for the point where its gradient is 0,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_w MSE_{train} &= 0 \\\\[0.2cm]\n",
    "\\nabla_w \\frac{1}{m} \\big\\| \\hat{y}^{(train)} - y^{(train)} \\big\\|^{2}_2 & = 0 \\\\[0.2cm]\n",
    "\\frac{1}{m} \\nabla_w \\big\\| X^{(train)}w - y^{(train)} \\big\\|^{2}_2 & = 0 \\\\[0.2cm]\n",
    "\\nabla_w \\bigl(X^{(train)}w - y^{(train)} \\bigl)^{T} \\bigl(X^{(train)}w - y^{(train)}) & = 0 \\\\[0.2cm]\n",
    "\\nabla_w \\bigl(w^{T}X^{(train)^{T}}X^{(train)}w - 2w^{T}X^{(train)^T}y^{(train)} + y^{(train)^{T}} y^{(train)} \\bigl) & = 0 \\\\[0.2cm]\n",
    "2 w (X^{(train)^T} X^{(train)}) - 2 (X^{(train)^T}y^{(train)}) & = 0 \\\\[0.2cm]\n",
    "\\therefore w & = (X^{(train)^{T}} X^{(train)})^{-1} \\cdot (X^{(train)^{T}}y^{(train)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is the normal equation for the linear model $\\hat{y} = w^{T}x$, from which we can derive the values of the parameters $w$, in simpler terms it takes the form,\n",
    "\n",
    "$$\n",
    "w = (X^{T}X)^{-1} \\cdot (X^{T}y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Normal Equation\n",
    "\n",
    "The above formula is for the linear model $\\hat{y} = w^{T}x$. But in our case we have taken a bias vector $b$, to which we assign $1$ for all entries, as is the usual practice.\n",
    "\n",
    "Here, the value of the matrix $X$ after adding the bias $b$ is,\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0 & 0 & 1\\\\\n",
    "0 & 1 & 1\\\\\n",
    "1 & 0 & 1\\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix} \\hspace{1cm}\n",
    "X^{T} = \\begin{bmatrix}\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X^{T}X = \\begin{bmatrix}\n",
    "2 & 1 & 2 \\\\\n",
    "1 & 2 & 2 \\\\\n",
    "2 & 2 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Computing $(X^{T}X)^{-1} \\cdot (X^{T}y)$, we get,\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.0 \\\\\n",
    "0.5 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The first two values corresponds to the parameters $w_1$ and $w_2$ and the third value is of the bias $b$. Hence, we get,\n",
    "$$\n",
    "w_1 = 0 \\\\\n",
    "w_2 = 0 \\\\\n",
    "b = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the linear model simply outputs $\\frac{1}{2}$ everywhere. Thus this linear model is not able to solve the problem of approximating the XOR function. To overcome this we will introduce a very simple feedforward network with one hidden layer containing two hidden units. This feedforward network has a vector of hidden units $h$ that are computed by a function $f^{1}(x; W, c)$.\n",
    "\n",
    "The values of these hidden units are then used as input for a second layer, which is the output layer of the network. The output layer is still just a linear regression model, but now it is applied to $h$ rather than $x$. The network now contains two functions chained together:\n",
    "$$\n",
    "h = f^{(1)}(x; W, c) \\\\\n",
    "y = f^{(2)}(h; w, b)\n",
    "$$\n",
    "\n",
    "The entire model is given by:\n",
    "\n",
    "$$\n",
    "f(x; W, c, w, b) = f^{(2)}(f^{(1)}(x))\n",
    "$$\n",
    "\n",
    "We will use a nonlinear function to describe the features. Most neural networks do so using an __affine__ transformation controlled by learned parameters, followed by a fixed nonlinear function called an activation function.\n",
    "\n",
    "We define $h = g(W^{T}x + c)$, where $W$ provides the weights of a linear transformation and $c$ the biases. The activation function h is typically chosen to be a function that is applied element wise, with  $h_i = g(x^{T}W_{:, i} + c)$.\n",
    "\n",
    "Here we will choose ReLU, which is given by, $g(z) = max\\bigl\\{0, z\\bigl\\}$. \n",
    "\n",
    "The complete neural network is then given by, \n",
    "\n",
    "$$\n",
    "f(x; W, c, w, b) = w^{T} max\\bigl\\{0, W^{T}x + c\\bigl\\} + b \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Network Based Solution to the XOR problem\n",
    "\n",
    "We are choosing the matrices and vectors $W$, $c$, $w$ and $b$ as below. In a real implementation these values would be randomized in the beggining and a gradiend-based learning method will gradually converge to these values during the training of the neural network. It should be noted that for this particular problem this set of parameters provides the global minimum. The convergence point of gradient descent depends on the initial values of the parameters. In practice, gradient descent would usually not find clean, easily understood, integer-valued solutions like the one presented here.\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix},\\\\[0.2cm]\n",
    "c = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "-1\n",
    "\\end{bmatrix},\\\\[0.2cm]\n",
    "w = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "-2\n",
    "\\end{bmatrix} \\\\[0.2cm]\n",
    "b = 0\n",
    "$$\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "The hidden layer performs the following operations:\n",
    "\n",
    "Operation 1: Multiply $X$ by $W$ to get $XW$\n",
    "\n",
    "Operation 2: Add the the bias vector $c$ to get $XW + c$\n",
    "\n",
    "Operation 3: Apply ReLU\n",
    "\n",
    "Operation 4: Resulting matrix from ReLU is multiplied by the weight matrix and passed on as output.\n",
    "\n",
    "The result due to these operations is as follows:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix} \\\\[0.2cm]\n",
    "\n",
    "\\implies XW = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\times\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1 \n",
    "\\end{bmatrix}\\\\[0.2cm]\n",
    "= \\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "2 & 2\n",
    "\\end{bmatrix}\\\\[0.2cm]\n",
    "\n",
    "\\implies XW + c  = \\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\\\\[0.2cm]\n",
    "\n",
    "\\implies ReLU(XW + c) = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 1 \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\\\[0.2cm]\n",
    "\n",
    "\\implies ReLU(XW + c) \\times w =\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "1 & 0 \\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\times\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "-2\n",
    "\\end{bmatrix}\\\\[0.2cm]\n",
    "\n",
    "\\therefore Output =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for computation of the normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2]\n",
      " [1 2 2]\n",
      " [2 2 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "X_t = np.array([[0, 0, 1, 1], [0, 1, 0, 1], [1, 1, 1, 1]])\n",
    "result = np.dot(X_t, X)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00 -0.00 -0.50]\n",
      " [ 0.00  1.00 -0.50]\n",
      " [-0.50 -0.50  0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse of (X_tX)\n",
    "prod_1 = np.linalg.inv(result)\n",
    "print(prod_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "# Computing (X_t)y\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "prod_2 = np.matmul(X_t, y)\n",
    "print(prod_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00]\n",
      " [0.00]\n",
      " [0.50]]\n"
     ]
    }
   ],
   "source": [
    "# Computing the parameter and bias vector,\n",
    "w = np.matmul(prod_1, prod_2)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6364572a05610c145f7348150b927d4800d35aff954ebf5201318569ffcf301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
