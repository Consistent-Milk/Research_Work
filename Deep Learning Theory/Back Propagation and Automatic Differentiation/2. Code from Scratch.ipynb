{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://github.com/Vercaca/NN-Backpropagation\n",
    "\n",
    "This code implements a neural network for the XOR function problem discussed earlier theoretically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ActivationFunction:\n",
    "\n",
    "    def __init__(self, types=\"sigmoid\"):\n",
    "        self.func = self.sigmoid\n",
    "        self.dfunc = self.dsigmoid\n",
    "\n",
    "        if types == 'sigmoid':\n",
    "            self.func = self.sigmoid\n",
    "            self.dfunc = self.dsigmoid\n",
    "    \n",
    "    def run(self):\n",
    "        return self.func(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        result = 1 / (1 + math.exp(-x))\n",
    "        return result\n",
    "    \n",
    "    # Derivative of sigmoid function\n",
    "    def dsigmoid(self, y):\n",
    "        return y*(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def backspace():\n",
    "    print('\\r', end='')                     # use '\\r' to go back\n",
    "\n",
    "def draw_progess_bar(n_finished, n_jobs, bar_length=30, sleep_time=0.0):\n",
    "    finish_percent = int(float((n_finished)) / n_jobs * 100)\n",
    "    progress_length = int(finish_percent * bar_length /100)\n",
    "    print (\"[%s>%s] %d%%\" % ('=' * progress_length, ' ' * (bar_length - progress_length), finish_percent), end='')\n",
    "    backspace()\n",
    "\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "STR_REPORT_BROADER = '+'+'-' * 60 + '+'\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, activ_func = 'Sigmoid', learning_rate='0.01', debug=True):\n",
    "        self.activ_func = ActivationFunction(types=activ_func)\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "\n",
    "    def add_layer(self, n_inputs, n_neurons):\n",
    "        layer = NeuralLayer(n_inputs, n_neurons, self.activ_func)\n",
    "        self.layers.append(layer)\n",
    "        return None\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inputs = layer.feed_forward(inputs) # next_input = previous_output\n",
    "\n",
    "            if self.debug:\n",
    "                print('Layer {}, Output: {}'.format(i+1, inputs))\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def feed_backward(self, targets): # backpropagating\n",
    "        if len(targets) != len(self.layers[-1].neurons):\n",
    "            raise Exception('wrong target numbers')\n",
    "\n",
    "        # calculate deltas of output layer\n",
    "        # Delta weight_ji = - (target_j - output_j) * deactivate_func(h_j) * input_i\n",
    "        for j, neuron_j in enumerate(self.layers[-1].neurons):\n",
    "            error = - (targets[j] - neuron_j.output)\n",
    "            neuron_j.calculate_delta(error)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Output_Layer: deltas: {}\".format(self.layers[-1].get_deltas()))\n",
    "\n",
    "\n",
    "        # calculate the hidden layers\n",
    "        n_hidden_layers = len(self.layers[:-1])\n",
    "        l = n_hidden_layers - 1\n",
    "\n",
    "        while l >= 0:\n",
    "            curr_layer, last_layer = self.layers[l], self.layers[l+1]\n",
    "\n",
    "            for i, neuron_i in enumerate(curr_layer.neurons):\n",
    "                # sum up the errors sent from the last layer\n",
    "                total_error = 0\n",
    "                for j, neuron_j in enumerate(last_layer.neurons):\n",
    "                    total_error += neuron_j.delta * neuron_j.weights[i] # total_error += delta_j * input_i_to_j\n",
    "\n",
    "                neuron_i.calculate_delta(total_error)\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"Layer {}: deltas: {}\".format(l+1, curr_layer.deltas))\n",
    "\n",
    "            l -= 1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def update_weights(self):\n",
    "        learning_rate = self.learning_rate\n",
    "        for l in self.layers:\n",
    "            l.update_weights(learning_rate)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def calculate_single_error(self, targets, actual_outputs):\n",
    "        error = 0\n",
    "        for i in range(len(targets)):\n",
    "            error += (targets[i] - actual_outputs[i]) ** 2\n",
    "        return error\n",
    "\n",
    "    def calculate_total_error(self, dataset):\n",
    "        \"\"\"\n",
    "        Return mean squared error of dataset\n",
    "        \"\"\"\n",
    "        total_error = 0\n",
    "        for inputs, targets in dataset:\n",
    "            actual_outputs = self.feed_forward(inputs)  # because you have to calculate the updated outputs, not = self.layers[-1].actual_outputs\n",
    "            total_error += self.calculate_single_error(targets, actual_outputs)\n",
    "\n",
    "        return total_error / len(dataset)\n",
    "\n",
    "\n",
    "    def train(self, dataset, n_iterations=100, print_error_report=True):\n",
    "\n",
    "        print('\\n> Training...')\n",
    "        print(STR_REPORT_BROADER)\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            print('| # {}/{}\\t| '.format(i+1, n_iterations), end=\"\", flush=True)\n",
    "            for j, (inputs, targets) in enumerate(dataset):\n",
    "                if self.debug:\n",
    "                    print('\\n>>> data #{}'.format(j+1))\n",
    "                self.feed_forward(inputs)\n",
    "                self.feed_backward(targets)\n",
    "                self.update_weights()\n",
    "            total_error = self.calculate_total_error(dataset)\n",
    "\n",
    "            if print_error_report:\n",
    "                print(' Total error: {}'.format(total_error))\n",
    "            else:\n",
    "                draw_progess_bar(n_finished=i+1, n_jobs=n_iterations, sleep_time=0.05) # draw progress bar\n",
    "\n",
    "        print('\\n' + STR_REPORT_BROADER)\n",
    "        print('Training Finish. Error = {}\\n'.format(total_error))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def test(self, dataset):\n",
    "        print('\\n> Testing...')\n",
    "        print(STR_REPORT_BROADER)\n",
    "\n",
    "        for j, (inputs, targets) in enumerate(dataset):\n",
    "            if self.debug:\n",
    "                print('\\n>>> data #{}'.format(j+1))\n",
    "\n",
    "            actual_outputs = self.feed_forward(inputs)\n",
    "            print('[#{}] {} -> {} (targets={})'.format(j, inputs, actual_outputs, targets))\n",
    "        total_error = self.calculate_total_error(dataset)\n",
    "\n",
    "        print(STR_REPORT_BROADER)\n",
    "        print('Testing Finish. Error: {}\\n'.format(total_error))\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "class NeuralLayer:\n",
    "    __counter = 0\n",
    "    def __init__(self, n_inputs, n_neurons, activ_func):\n",
    "        self.__counter = NeuralLayer.__counter = NeuralLayer.__counter + 1\n",
    "        self.__neurons = [Neuron(n_inputs, activ_func) for _ in range(n_neurons)]\n",
    "\n",
    "    @property\n",
    "    def neurons(self):\n",
    "        return self.__neurons\n",
    "\n",
    "    # @property\n",
    "    # def actual_outputs(self):\n",
    "    #     return [neuron.output for neuron in self.neurons]\n",
    "\n",
    "    @property\n",
    "    def deltas(self):\n",
    "        return [i.delta for i in self.neurons]\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        return [neuron.calculate_output(inputs) for neuron in self.neurons]\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(learning_rate)\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '-- Layer {}  # of neurons: {}'.format(self.__counter, len(self.neurons))\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_weights, activ_func='Sigmoid', bias=1):\n",
    "        self.__weights = [random.random() for i in range(n_weights)] # np.random.rand(n_weights)#\n",
    "        self.__bias = bias\n",
    "        self.__output = 0.0\n",
    "        self.__inputs = []\n",
    "        self.__delta = 0.0\n",
    "        self.__n_weights = n_weights\n",
    "        self.__activation = activ_func\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.__output\n",
    "    @property\n",
    "    def delta(self):\n",
    "        return self.__delta\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        n_weights = self.__n_weights\n",
    "        if len(inputs) != n_weights:\n",
    "            raise Exception('wrong inputs number')\n",
    "\n",
    "        output = sum([inputs[i] * self.__weights[i] for i in range(n_weights)])\n",
    "        a_output = self.__activation.func(output + self.__bias)\n",
    "\n",
    "        # set the variables\n",
    "        self.__inputs = inputs\n",
    "        self.__output = a_output\n",
    "\n",
    "        return a_output\n",
    "\n",
    "    # Backpropagation step \n",
    "    def calculate_delta(self, error):\n",
    "        self.__delta = error * self.__activation.dfunc(self.__output)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for i in range(self.__n_weights):\n",
    "            self.__weights[i] -= learning_rate * self.__delta * self.__inputs[i]\n",
    "        self.__bias -= learning_rate * self.__delta\n",
    "\n",
    "        # update output\n",
    "        self.calculate_output(self.__inputs)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '--- weights = {}, bias = {}'.format(self.__weights, self.__bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Training...\n",
      "+------------------------------------------------------------+\n",
      "| # 1/100\t|  Total error: 0.36870144224023726\n",
      "| # 2/100\t|  Total error: 0.36377778585679166\n",
      "| # 3/100\t|  Total error: 0.35870452611927084\n",
      "| # 4/100\t|  Total error: 0.3534975078684244\n",
      "| # 5/100\t|  Total error: 0.3481766273861199\n",
      "| # 6/100\t|  Total error: 0.34276591343268914\n",
      "| # 7/100\t|  Total error: 0.33729342225730247\n",
      "| # 8/100\t|  Total error: 0.3317909101438412\n",
      "| # 9/100\t|  Total error: 0.32629326011822923\n",
      "| # 10/100\t|  Total error: 0.32083765950939075\n",
      "| # 11/100\t|  Total error: 0.3154625507772196\n",
      "| # 12/100\t|  Total error: 0.3102064063679002\n",
      "| # 13/100\t|  Total error: 0.3051064048665365\n",
      "| # 14/100\t|  Total error: 0.30019710528152616\n",
      "| # 15/100\t|  Total error: 0.29550922437897986\n",
      "| # 16/100\t|  Total error: 0.2910686159350704\n",
      "| # 17/100\t|  Total error: 0.2868955307584622\n",
      "| # 18/100\t|  Total error: 0.2830042055212493\n",
      "| # 19/100\t|  Total error: 0.279402792269528\n",
      "| # 20/100\t|  Total error: 0.27609360528692745\n",
      "| # 21/100\t|  Total error: 0.2730736333695962\n",
      "| # 22/100\t|  Total error: 0.2703352472001788\n",
      "| # 23/100\t|  Total error: 0.26786702454511613\n",
      "| # 24/100\t|  Total error: 0.26565461931625134\n",
      "| # 25/100\t|  Total error: 0.2636816114531385\n",
      "| # 26/100\t|  Total error: 0.2619302897941077\n",
      "| # 27/100\t|  Total error: 0.2603823365046236\n",
      "| # 28/100\t|  Total error: 0.2590193968321165\n",
      "| # 29/100\t|  Total error: 0.25782353048495305\n",
      "| # 30/100\t|  Total error: 0.25677755017508025\n",
      "| # 31/100\t|  Total error: 0.2558652588582739\n",
      "| # 32/100\t|  Total error: 0.2550716003983125\n",
      "| # 33/100\t|  Total error: 0.25438273940234923\n",
      "| # 34/100\t|  Total error: 0.2537860854778738\n",
      "| # 35/100\t|  Total error: 0.25327027572554295\n",
      "| # 36/100\t|  Total error: 0.2528251273656767\n",
      "| # 37/100\t|  Total error: 0.25244157033074366\n",
      "| # 38/100\t|  Total error: 0.25211156765908144\n",
      "| # 39/100\t|  Total error: 0.251828029722706\n",
      "| # 40/100\t|  Total error: 0.2515847267739664\n",
      "| # 41/100\t|  Total error: 0.2513762030166672\n",
      "| # 42/100\t|  Total error: 0.25119769438419615\n",
      "| # 43/100\t|  Total error: 0.25104505141239164\n",
      "| # 44/100\t|  Total error: 0.250914667994397\n",
      "| # 45/100\t|  Total error: 0.25080341636394227\n",
      "| # 46/100\t|  Total error: 0.2507085883401409\n",
      "| # 47/100\t|  Total error: 0.2506278426527632\n",
      "| # 48/100\t|  Total error: 0.2505591580282989\n",
      "| # 49/100\t|  Total error: 0.2505007916346036\n",
      "| # 50/100\t|  Total error: 0.2504512424401555\n",
      "| # 51/100\t|  Total error: 0.2504092190309848\n",
      "| # 52/100\t|  Total error: 0.25037361143505754\n",
      "| # 53/100\t|  Total error: 0.2503434665234857\n",
      "| # 54/100\t|  Total error: 0.25031796658537303\n",
      "| # 55/100\t|  Total error: 0.2502964107048016\n",
      "| # 56/100\t|  Total error: 0.25027819860185974\n",
      "| # 57/100\t|  Total error: 0.2502628166329977\n",
      "| # 58/100\t|  Total error: 0.25024982567820936\n",
      "| # 59/100\t|  Total error: 0.250238850672894\n",
      "| # 60/100\t|  Total error: 0.2502295715703362\n",
      "| # 61/100\t|  Total error: 0.25022171554637934\n",
      "| # 62/100\t|  Total error: 0.2502150502810353\n",
      "| # 63/100\t|  Total error: 0.25020937817251987\n",
      "| # 64/100\t|  Total error: 0.25020453135766857\n",
      "| # 65/100\t|  Total error: 0.25020036742902696\n",
      "| # 66/100\t|  Total error: 0.25019676575330857\n",
      "| # 67/100\t|  Total error: 0.2501936243085466\n",
      "| # 68/100\t|  Total error: 0.25019085696832355\n",
      "| # 69/100\t|  Total error: 0.25018839117111047\n",
      "| # 70/100\t|  Total error: 0.2501861659211517\n",
      "| # 71/100\t|  Total error: 0.25018413007463236\n",
      "| # 72/100\t|  Total error: 0.25018224087120355\n",
      "| # 73/100\t|  Total error: 0.2501804626764329\n",
      "| # 74/100\t|  Total error: 0.25017876590550203\n",
      "| # 75/100\t|  Total error: 0.2501771261025829\n",
      "| # 76/100\t|  Total error: 0.2501755231538777\n",
      "| # 77/100\t|  Total error: 0.2501739406153731\n",
      "| # 78/100\t|  Total error: 0.2501723651390075\n",
      "| # 79/100\t|  Total error: 0.250170785983227\n",
      "| # 80/100\t|  Total error: 0.25016919459587905\n",
      "| # 81/100\t|  Total error: 0.25016758425907965\n",
      "| # 82/100\t|  Total error: 0.250165949787156\n",
      "| # 83/100\t|  Total error: 0.25016428727001494\n",
      "| # 84/100\t|  Total error: 0.2501625938553767\n",
      "| # 85/100\t|  Total error: 0.2501608675642343\n",
      "| # 86/100\t|  Total error: 0.25015910713470785\n",
      "| # 87/100\t|  Total error: 0.2501573118901422\n",
      "| # 88/100\t|  Total error: 0.25015548162789325\n",
      "| # 89/100\t|  Total error: 0.2501536165257549\n",
      "| # 90/100\t|  Total error: 0.2501517170634118\n",
      "| # 91/100\t|  Total error: 0.25014978395668414\n",
      "| # 92/100\t|  Total error: 0.2501478181026455\n",
      "| # 93/100\t|  Total error: 0.25014582053397844\n",
      "| # 94/100\t|  Total error: 0.2501437923811627\n",
      "| # 95/100\t|  Total error: 0.2501417348413001\n",
      "| # 96/100\t|  Total error: 0.25013964915255243\n",
      "| # 97/100\t|  Total error: 0.25013753657331733\n",
      "| # 98/100\t|  Total error: 0.2501353983653978\n",
      "| # 99/100\t|  Total error: 0.2501332357805298\n",
      "| # 100/100\t|  Total error: 0.25013105004972647\n",
      "\n",
      "+------------------------------------------------------------+\n",
      "Training Finish. Error = 0.25013105004972647\n",
      "\n",
      "\n",
      "> Testing...\n",
      "+------------------------------------------------------------+\n",
      "[#0] (1, 0) -> [0.5010599269257985] (targets=[1])\n",
      "[#1] (0, 0) -> [0.506883537026416] (targets=[0])\n",
      "+------------------------------------------------------------+\n",
      "Testing Finish. Error: 0.2529360583138498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of XOR\n",
    "train_dataset = [\n",
    "            [(1, 0), [1]],\n",
    "            [(0, 0), [0]],\n",
    "            [(0, 1), [1]],\n",
    "            [(1, 1), [0]]\n",
    "            ]\n",
    "\n",
    "nn = NeuralNetwork(learning_rate=0.1, debug=False)\n",
    "nn.add_layer(n_inputs=2, n_neurons=3)\n",
    "nn.add_layer(n_inputs=3, n_neurons=1)\n",
    "\n",
    "nn.train(dataset=train_dataset, n_iterations=100, print_error_report=True)\n",
    "\n",
    "# test\n",
    "test_dataset = [\n",
    "    [(1, 0), [1]],\n",
    "    [(0, 0), [0]]\n",
    "]\n",
    "nn.test(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6364572a05610c145f7348150b927d4800d35aff954ebf5201318569ffcf301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
