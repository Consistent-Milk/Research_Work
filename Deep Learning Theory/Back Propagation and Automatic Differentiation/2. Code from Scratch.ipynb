{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://github.com/Vercaca/NN-Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ActivationFunction:\n",
    "\n",
    "    def __init__(self, types=\"sigmoid\"):\n",
    "        self.func = self.sigmoid\n",
    "        self.dfunc = self.dsigmoid\n",
    "\n",
    "        if types == 'sigmoid':\n",
    "            self.func = self.sigmoid\n",
    "            self.dfunc = self.dsigmoid\n",
    "    \n",
    "    def run(self):\n",
    "        return self.func(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        result = 1 / (1 + math.exp(-x))\n",
    "        return result\n",
    "    \n",
    "    # Derivative of sigmoid function\n",
    "    def dsigmoid(self, y):\n",
    "        return y*(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def backspace():\n",
    "    print('\\r', end='')                     # use '\\r' to go back\n",
    "\n",
    "def draw_progess_bar(n_finished, n_jobs, bar_length=30, sleep_time=0.0):\n",
    "    finish_percent = int(float((n_finished)) / n_jobs * 100)\n",
    "    progress_length = int(finish_percent * bar_length /100)\n",
    "    print (\"[%s>%s] %d%%\" % ('=' * progress_length, ' ' * (bar_length - progress_length), finish_percent), end='')\n",
    "    backspace()\n",
    "\n",
    "    time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "STR_REPORT_BROADER = '+'+'-' * 60 + '+'\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, activ_func = 'Sigmoid', learning_rate='0.01', debug=True):\n",
    "        self.activ_func = ActivationFunction(types=activ_func)\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.debug = debug\n",
    "\n",
    "    def add_layer(self, n_inputs, n_neurons):\n",
    "        layer = NeuralLayer(n_inputs, n_neurons, self.activ_func)\n",
    "        self.layers.append(layer)\n",
    "        return None\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inputs = layer.feed_forward(inputs) # next_input = previous_output\n",
    "\n",
    "            if self.debug:\n",
    "                print('Layer {}, Output: {}'.format(i+1, inputs))\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def feed_backward(self, targets): # backpropagating\n",
    "        if len(targets) != len(self.layers[-1].neurons):\n",
    "            raise Exception('wrong target numbers')\n",
    "\n",
    "        # calculate deltas of output layer\n",
    "        # Delta weight_ji = - (target_j - output_j) * deactivate_func(h_j) * input_i\n",
    "        for j, neuron_j in enumerate(self.layers[-1].neurons):\n",
    "            error = - (targets[j] - neuron_j.output)\n",
    "            neuron_j.calculate_delta(error)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Output_Layer: deltas: {}\".format(self.layers[-1].get_deltas()))\n",
    "\n",
    "\n",
    "        # calculate the hidden layers\n",
    "        n_hidden_layers = len(self.layers[:-1])\n",
    "        l = n_hidden_layers - 1\n",
    "\n",
    "        while l >= 0:\n",
    "            curr_layer, last_layer = self.layers[l], self.layers[l+1]\n",
    "\n",
    "            for i, neuron_i in enumerate(curr_layer.neurons):\n",
    "                # sum up the errors sent from the last layer\n",
    "                total_error = 0\n",
    "                for j, neuron_j in enumerate(last_layer.neurons):\n",
    "                    total_error += neuron_j.delta * neuron_j.weights[i] # total_error += delta_j * input_i_to_j\n",
    "\n",
    "                neuron_i.calculate_delta(total_error)\n",
    "\n",
    "            if self.debug:\n",
    "                print(\"Layer {}: deltas: {}\".format(l+1, curr_layer.deltas))\n",
    "\n",
    "            l -= 1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def update_weights(self):\n",
    "        learning_rate = self.learning_rate\n",
    "        for l in self.layers:\n",
    "            l.update_weights(learning_rate)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def calculate_single_error(self, targets, actual_outputs):\n",
    "        error = 0\n",
    "        for i in range(len(targets)):\n",
    "            error += (targets[i] - actual_outputs[i]) ** 2\n",
    "        return error\n",
    "\n",
    "    def calculate_total_error(self, dataset):\n",
    "        \"\"\"\n",
    "        Return mean squared error of dataset\n",
    "        \"\"\"\n",
    "        total_error = 0\n",
    "        for inputs, targets in dataset:\n",
    "            actual_outputs = self.feed_forward(inputs)  # because you have to calculate the updated outputs, not = self.layers[-1].actual_outputs\n",
    "            total_error += self.calculate_single_error(targets, actual_outputs)\n",
    "\n",
    "        return total_error / len(dataset)\n",
    "\n",
    "\n",
    "    def train(self, dataset, n_iterations=100, print_error_report=True):\n",
    "\n",
    "        print('\\n> Training...')\n",
    "        print(STR_REPORT_BROADER)\n",
    "\n",
    "        for i in range(n_iterations):\n",
    "            print('| # {}/{}\\t| '.format(i+1, n_iterations), end=\"\", flush=True)\n",
    "            for j, (inputs, targets) in enumerate(dataset):\n",
    "                if self.debug:\n",
    "                    print('\\n>>> data #{}'.format(j+1))\n",
    "                self.feed_forward(inputs)\n",
    "                self.feed_backward(targets)\n",
    "                self.update_weights()\n",
    "            total_error = self.calculate_total_error(dataset)\n",
    "\n",
    "            if print_error_report:\n",
    "                print(' Total error: {}'.format(total_error))\n",
    "            else:\n",
    "                draw_progess_bar(n_finished=i+1, n_jobs=n_iterations, sleep_time=0.05) # draw progress bar\n",
    "\n",
    "        print('\\n' + STR_REPORT_BROADER)\n",
    "        print('Training Finish. Error = {}\\n'.format(total_error))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def test(self, dataset):\n",
    "        print('\\n> Testing...')\n",
    "        print(STR_REPORT_BROADER)\n",
    "\n",
    "        for j, (inputs, targets) in enumerate(dataset):\n",
    "            if self.debug:\n",
    "                print('\\n>>> data #{}'.format(j+1))\n",
    "\n",
    "            actual_outputs = self.feed_forward(inputs)\n",
    "            print('[#{}] {} -> {} (targets={})'.format(j, inputs, actual_outputs, targets))\n",
    "        total_error = self.calculate_total_error(dataset)\n",
    "\n",
    "        print(STR_REPORT_BROADER)\n",
    "        print('Testing Finish. Error: {}\\n'.format(total_error))\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "class NeuralLayer:\n",
    "    __counter = 0\n",
    "    def __init__(self, n_inputs, n_neurons, activ_func):\n",
    "        self.__counter = NeuralLayer.__counter = NeuralLayer.__counter + 1\n",
    "        self.__neurons = [Neuron(n_inputs, activ_func) for _ in range(n_neurons)]\n",
    "\n",
    "    @property\n",
    "    def neurons(self):\n",
    "        return self.__neurons\n",
    "\n",
    "    # @property\n",
    "    # def actual_outputs(self):\n",
    "    #     return [neuron.output for neuron in self.neurons]\n",
    "\n",
    "    @property\n",
    "    def deltas(self):\n",
    "        return [i.delta for i in self.neurons]\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        return [neuron.calculate_output(inputs) for neuron in self.neurons]\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.update_weights(learning_rate)\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '-- Layer {}  # of neurons: {}'.format(self.__counter, len(self.neurons))\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_weights, activ_func='Sigmoid', bias=1):\n",
    "        self.__weights = [random.random() for i in range(n_weights)] # np.random.rand(n_weights)#\n",
    "        self.__bias = bias\n",
    "        self.__output = 0.0\n",
    "        self.__inputs = []\n",
    "        self.__delta = 0.0\n",
    "        self.__n_weights = n_weights\n",
    "        self.__activation = activ_func\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.__output\n",
    "    @property\n",
    "    def delta(self):\n",
    "        return self.__delta\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        n_weights = self.__n_weights\n",
    "        if len(inputs) != n_weights:\n",
    "            raise Exception('wrong inputs number')\n",
    "\n",
    "        output = sum([inputs[i] * self.__weights[i] for i in range(n_weights)])\n",
    "        a_output = self.__activation.func(output + self.__bias)\n",
    "\n",
    "        # set the variables\n",
    "        self.__inputs = inputs\n",
    "        self.__output = a_output\n",
    "\n",
    "        return a_output\n",
    "\n",
    "    def calculate_delta(self, error):\n",
    "        self.__delta = error * self.__activation.dfunc(self.__output)\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        for i in range(self.__n_weights):\n",
    "            self.__weights[i] -= learning_rate * self.__delta * self.__inputs[i]\n",
    "        self.__bias -= learning_rate * self.__delta\n",
    "\n",
    "        # update output\n",
    "        self.calculate_output(self.__inputs)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '--- weights = {}, bias = {}'.format(self.__weights, self.__bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Training...\n",
      "+------------------------------------------------------------+\n",
      "| # 1/100\t|  Total error: 0.4505130685772799\n",
      "| # 2/100\t|  Total error: 0.4493256637958525\n",
      "| # 3/100\t|  Total error: 0.44808802121889213\n",
      "| # 4/100\t|  Total error: 0.44679730465998635\n",
      "| # 5/100\t|  Total error: 0.4454504951382429\n",
      "| # 6/100\t|  Total error: 0.4440443799726953\n",
      "| # 7/100\t|  Total error: 0.4425755417502065\n",
      "| # 8/100\t|  Total error: 0.4410403473369237\n",
      "| # 9/100\t|  Total error: 0.43943493715880755\n",
      "| # 10/100\t|  Total error: 0.4377552150455826\n",
      "| # 11/100\t|  Total error: 0.4359968390173107\n",
      "| # 12/100\t|  Total error: 0.43415521349668296\n",
      "| # 13/100\t|  Total error: 0.4322254835564277\n",
      "| # 14/100\t|  Total error: 0.430202531963512\n",
      "| # 15/100\t|  Total error: 0.42808097996377603\n",
      "| # 16/100\t|  Total error: 0.4258551929657184\n",
      "| # 17/100\t|  Total error: 0.4235192925331639\n",
      "| # 18/100\t|  Total error: 0.4210671763849359\n",
      "| # 19/100\t|  Total error: 0.41849254842448086\n",
      "| # 20/100\t|  Total error: 0.4157889611789322\n",
      "| # 21/100\t|  Total error: 0.41294987340484535\n",
      "| # 22/100\t|  Total error: 0.4099687259979686\n",
      "| # 23/100\t|  Total error: 0.40683903969652213\n",
      "| # 24/100\t|  Total error: 0.4035545383457\n",
      "| # 25/100\t|  Total error: 0.4001093016298807\n",
      "| # 26/100\t|  Total error: 0.3964979510887879\n",
      "| # 27/100\t|  Total error: 0.39271587279798126\n",
      "| # 28/100\t|  Total error: 0.3887594791683612\n",
      "| # 29/100\t|  Total error: 0.38462651073669474\n",
      "| # 30/100\t|  Total error: 0.38031637640221194\n",
      "| # 31/100\t|  Total error: 0.37583052715068194\n",
      "| # 32/100\t|  Total error: 0.3711728537904999\n",
      "| # 33/100\t|  Total error: 0.36635009361344617\n",
      "| # 34/100\t|  Total error: 0.3613722243843411\n",
      "| # 35/100\t|  Total error: 0.35625281712983586\n",
      "| # 36/100\t|  Total error: 0.3510093126506887\n",
      "| # 37/100\t|  Total error: 0.34566318170576726\n",
      "| # 38/100\t|  Total error: 0.34023992689859167\n",
      "| # 39/100\t|  Total error: 0.33476888704647856\n",
      "| # 40/100\t|  Total error: 0.3292828136190443\n",
      "| # 41/100\t|  Total error: 0.32381720441097106\n",
      "| # 42/100\t|  Total error: 0.31840940151145514\n",
      "| # 43/100\t|  Total error: 0.3130974868748773\n",
      "| # 44/100\t|  Total error: 0.307919035858628\n",
      "| # 45/100\t|  Total error: 0.3029098123500333\n",
      "| # 46/100\t|  Total error: 0.29810250377058267\n",
      "| # 47/100\t|  Total error: 0.29352559660903\n",
      "| # 48/100\t|  Total error: 0.28920248169714413\n",
      "| # 49/100\t|  Total error: 0.28515085454560724\n",
      "| # 50/100\t|  Total error: 0.28138244370746845\n",
      "| # 51/100\t|  Total error: 0.2779030650229459\n",
      "| # 52/100\t|  Total error: 0.27471296764758346\n",
      "| # 53/100\t|  Total error: 0.2718074137116041\n",
      "| # 54/100\t|  Total error: 0.26917741990467686\n",
      "| # 55/100\t|  Total error: 0.2668105864620973\n",
      "| # 56/100\t|  Total error: 0.2646919452274513\n",
      "| # 57/100\t|  Total error: 0.26280477081392906\n",
      "| # 58/100\t|  Total error: 0.2611313142281474\n",
      "| # 59/100\t|  Total error: 0.2596534339010721\n",
      "| # 60/100\t|  Total error: 0.25835311291474833\n",
      "| # 61/100\t|  Total error: 0.2572128622164555\n",
      "| # 62/100\t|  Total error: 0.25621601742548195\n",
      "| # 63/100\t|  Total error: 0.2553469416545062\n",
      "| # 64/100\t|  Total error: 0.2545911490904122\n",
      "| # 65/100\t|  Total error: 0.25393536452943133\n",
      "| # 66/100\t|  Total error: 0.2533675332455091\n",
      "| # 67/100\t|  Total error: 0.25287679400625906\n",
      "| # 68/100\t|  Total error: 0.25245342613659183\n",
      "| # 69/100\t|  Total error: 0.25208877954653663\n",
      "| # 70/100\t|  Total error: 0.25177519476542887\n",
      "| # 71/100\t|  Total error: 0.25150591835870795\n",
      "| # 72/100\t|  Total error: 0.251275017688469\n",
      "| # 73/100\t|  Total error: 0.2510772978196797\n",
      "| # 74/100\t|  Total error: 0.2509082224533279\n",
      "| # 75/100\t|  Total error: 0.25076384005699154\n",
      "| # 76/100\t|  Total error: 0.25064071582952746\n",
      "| # 77/100\t|  Total error: 0.25053586974716807\n",
      "| # 78/100\t|  Total error: 0.25044672066353413\n",
      "| # 79/100\t|  Total error: 0.25037103625009965\n",
      "| # 80/100\t|  Total error: 0.25030688844496074\n",
      "| # 81/100\t|  Total error: 0.25025261400889826\n",
      "| # 82/100\t|  Total error: 0.25020677975488687\n",
      "| # 83/100\t|  Total error: 0.25016815200969056\n",
      "| # 84/100\t|  Total error: 0.25013566987586566\n",
      "| # 85/100\t|  Total error: 0.2501084218832925\n",
      "| # 86/100\t|  Total error: 0.2500856256468358\n",
      "| # 87/100\t|  Total error: 0.2500666101776923\n",
      "| # 88/100\t|  Total error: 0.2500508005281885\n",
      "| # 89/100\t|  Total error: 0.2500377044817207\n",
      "| # 90/100\t|  Total error: 0.25002690103019215\n",
      "| # 91/100\t|  Total error: 0.25001803041008686\n",
      "| # 92/100\t|  Total error: 0.2500107854948963\n",
      "| # 93/100\t|  Total error: 0.2500049043658397\n",
      "| # 94/100\t|  Total error: 0.2500001639046805\n",
      "| # 95/100\t|  Total error: 0.2499963742720124\n",
      "| # 96/100\t|  Total error: 0.24999337415180212\n",
      "| # 97/100\t|  Total error: 0.24999102665837858\n",
      "| # 98/100\t|  Total error: 0.2499892158156361\n",
      "| # 99/100\t|  Total error: 0.2499878435301362\n",
      "| # 100/100\t|  Total error: 0.2499868269902218\n",
      "\n",
      "+------------------------------------------------------------+\n",
      "Training Finish. Error = 0.2499868269902218\n",
      "\n",
      "\n",
      "> Testing...\n",
      "+------------------------------------------------------------+\n",
      "[#0] (1, 0) -> [0.5031595932994741] (targets=[1])\n",
      "[#1] (0, 0) -> [0.5029836692415443] (targets=[0])\n",
      "+------------------------------------------------------------+\n",
      "Testing Finish. Error: 0.24992148062701558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of XOR\n",
    "train_dataset = [\n",
    "            [(1, 0), [1]],\n",
    "            [(0, 0), [0]],\n",
    "            [(0, 1), [1]],\n",
    "            [(1, 1), [0]]\n",
    "            ]\n",
    "\n",
    "nn = NeuralNetwork(learning_rate=0.1, debug=False)\n",
    "nn.add_layer(n_inputs=2, n_neurons=3)\n",
    "nn.add_layer(n_inputs=3, n_neurons=1)\n",
    "\n",
    "nn.train(dataset=train_dataset, n_iterations=100, print_error_report=True)\n",
    "\n",
    "# test\n",
    "test_dataset = [\n",
    "    [(1, 0), [1]],\n",
    "    [(0, 0), [0]]\n",
    "]\n",
    "nn.test(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6364572a05610c145f7348150b927d4800d35aff954ebf5201318569ffcf301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
